## LangChain4j Ollama configuration
quarkus.langchain4j.ollama.chat-model.model-id=llama3:8b
quarkus.langchain4j.ollama.chat-model.temperature=0.5
quarkus.langchain4j.ollama.timeout=180s

# Default memory type
quarkus.langchain4j.chat-memory.type=MESSAGE_WINDOW
quarkus.langchain4j.chat-memory.memory-window.max-messages=10

